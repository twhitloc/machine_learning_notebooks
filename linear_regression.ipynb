{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8feea8d0-3cc2-490c-9ae0-fc04c7e7d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cdf2b0-c511-4dbf-aecc-d36e9d99f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join('Users',\n",
    "                         'tylerwhitlock'\n",
    "                         'Development',\n",
    "                         'machine_learning_algorithms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b867ab-395e-41e0-adbf-1d5783ba1246",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aea546-d098-4226-ab21-81d79523fb96",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fd750-e5e4-41ee-ae88-ccd77c35eff8",
   "metadata": {},
   "source": [
    "### What is Linear Regression\n",
    "\n",
    "Linear Regression is a machine learning algorithm that comes from statistics that seeks to fit a scalar of values, called the <b>Dependent Variable </b> given a matrix of features, known as the <b>Independent Variables</b>.  Another way to think of this is fitting the <b>y values</b> given the <b>x values</b>\n",
    "\n",
    "* $$y_{i} = β_{0} + β_{1} x _{i 1} + ⋯ + β _{p} x _{i p} + ε i = x _{i} ^{T} β + ε _{i} , i = 1 , … , n , $$\n",
    "\n",
    "Linear Regresssion is trained using a method known as gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff448bb5-0304-42bc-b4b0-27cad2b828e8",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Gradient Descent is an optimization method for finding a given set of weights to multiply by our independent variables that will give us the solution, our dependent variable\n",
    "\n",
    "Gradient Descent is an iterative process by which we apply the Error / Cost function to a given set of examples to determine this set of weights that optimally solves for these examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28839803-7b7d-4d64-9dea-78f7b614ec92",
   "metadata": {},
   "source": [
    "## Cost / Error Functions\n",
    "For Linear Regression we seek to find a given set of weights, or biases that will, when multiplied by the independent variables give us the dependent variable y.\n",
    "\n",
    "In the case of Linear Regression then we will be trying to find a line of best fit for the sets of independent variables in Ordinary Least Squares in the given training sample.  \n",
    "We can do this using a few methods:\n",
    "\n",
    "### Error Functions for Linear Regression\n",
    "    * Ordinary Least Squares\n",
    "    \n",
    "    * Regularized Regressions:  These prevent colinearities, or highly correlated features, from overpowering the model\n",
    "        * Ridge Regression, a technique that adds regularization, or bias to the Ordinary Least Squares Calculation\n",
    "        * Lasso Regression, a technique that adds regularization and variable selection to the process of the Least Squares Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909fb1e-48f4-4af6-9ee3-996728d7bd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
