{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8feea8d0-3cc2-490c-9ae0-fc04c7e7d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "24cdf2b0-c511-4dbf-aecc-d36e9d99f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join('Users',\n",
    "                         'tylerwhitlock'\n",
    "                         'Development',\n",
    "                         'machine_learning_algorithms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b867ab-395e-41e0-adbf-1d5783ba1246",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aea546-d098-4226-ab21-81d79523fb96",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fd750-e5e4-41ee-ae88-ccd77c35eff8",
   "metadata": {},
   "source": [
    "### What is Linear Regression\n",
    "\n",
    "Linear Regression is a machine learning algorithm that comes from statistics that seeks to fit a scalar of values, called the <b>Dependent Variable </b> given a matrix of features, known as the <b>Independent Variables</b>.  Another way to think of this is fitting the <b>y values</b> given the <b>x values</b>\n",
    "\n",
    "* $$y_{i} = β_{0} + β_{1} x _{i 1} + ⋯ + β _{p} x _{i p} + ε i = x _{i} ^{T} β + ε _{i} , i = 1 , … , n , $$\n",
    "\n",
    "Linear Regresssion is trained using a method known as gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff448bb5-0304-42bc-b4b0-27cad2b828e8",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Gradient Descent is an optimization method for finding a given set of weights to multiply by our independent variables that will give us the solution, our dependent variable\n",
    "\n",
    "Gradient Descent is an iterative process by which we apply the Error / Cost function to a given set of examples to determine this set of weights that optimally solves for these examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28839803-7b7d-4d64-9dea-78f7b614ec92",
   "metadata": {},
   "source": [
    "## Cost / Error Functions\n",
    "For Linear Regression we seek to find a given set of weights, or biases that will, when multiplied by the independent variables give us the dependent variable y.\n",
    "\n",
    "In the case of Linear Regression then we will be trying to find a line of best fit for the sets of independent variables in Ordinary Least Squares in the given training sample.  \n",
    "We can do this using a few methods:\n",
    "\n",
    "### Error Functions for Linear Regression\n",
    "    * Ordinary Least Squares\n",
    "    \n",
    "    * Regularized Regressions:  These prevent colinearities, or highly correlated features, from overpowering the model\n",
    "        * Ridge Regression, a technique that adds regularization, or bias to the Ordinary Least Squares Calculation\n",
    "        * Lasso Regression, a technique that adds regularization and variable selection to the process of the Least Squares Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82f07a-3dd7-4033-9f21-0041d3afae7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "db0d8d27-8731-4578-8001-37e641156924",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 25\n",
    "X, y, coefficients = make_regression(n_samples=1000,\n",
    "                                     n_features=n_features,\n",
    "                                     n_informative=15,\n",
    "                                     n_targets=1,\n",
    "                                     random_state=1,\n",
    "                                     coef=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dadf5dc6-cb98-44c3-a9c4-530c07e3b43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our expected results of regression should be approximately these given the randomly generated dataset\n",
      "\n",
      "Coefficients: \n",
      " [72.6895 26.8971 38.974  21.7287 82.0443  0.     32.2727  0.     76.0003\n",
      "  0.     10.7963 75.4174  0.     58.2359  0.      0.      0.     86.4786\n",
      " 28.8514  0.     72.0621 68.1451  0.     80.165   0.    ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Our expected results of regression should be approximately these given the randomly generated dataset\")\n",
    "print(f\"\\nCoefficients: \\n {coefficients}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "03d357f9-5818-41ae-82a6-27e1f0a47eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,\n",
    "                 num_features,\n",
    "                 learning_rate=0.003,\n",
    "                 num_iterations=1500):\n",
    "        \n",
    "        self.beta = np.random.random()\n",
    "        self.num_features = num_features\n",
    "        self.num_iterations = num_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.random((num_features,))\n",
    "        self.record = []\n",
    "        \n",
    "    def compute_error(self, X, y):\n",
    "        num_examples = X.shape[0]\n",
    "        error = 0\n",
    "        for i in range(num_examples):\n",
    "            prediction = self.predict(X[i])\n",
    "            error = error + ((prediction - y[i]) ** 2)\n",
    "        error = error/(2*num_examples)\n",
    "        return error\n",
    "    \n",
    "    def compute_gradient(self, X, y):\n",
    "        num_examples, num_features = X.shape\n",
    "        weights_derivative = np.zeros(num_features)\n",
    "        beta_derivative = 0.0\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            error = (self.predict(X[i]) - y[i])\n",
    "            for j in range(num_features):\n",
    "                weights_derivative[j] = weights_derivative[j] + error * X[i,j]\n",
    "            beta_derivative += error\n",
    "        beta_derivative /= num_examples\n",
    "        weights_derivative /= num_examples\n",
    "        \n",
    "        return weights_derivative, beta_derivative\n",
    "    \n",
    "    def gradient_descent(self, X, y):\n",
    "        \n",
    "        for iter_ct in range(self.num_iterations):\n",
    "            weight_derivatives, beta_derivative = self.compute_gradient(X, y)\n",
    "            self.weights = self.weights - (self.learning_rate * weight_derivatives)\n",
    "            self.beta = self.beta - (self.learning_rate * beta_derivative)\n",
    "            self.record.append(self.compute_error(X, y))\n",
    "            \n",
    "            if iter_ct % 100 == 0:\n",
    "                print(f\"Iteration {iter_ct} : cost {self.record[iter_ct]}\")\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        record = self.gradient_descent(X, y)\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction = np.dot(X, self.weights) + self.beta\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6ebf3b1f-cf26-4108-a4a2-42a765f0147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = LinearRegressor(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e796728-fab1-4b65-9a7c-8958405732af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : cost 24952.903649230284\n",
      "Iteration 100 : cost 14202.844405788557\n",
      "Iteration 200 : cost 8152.143970133232\n",
      "Iteration 300 : cost 4716.218238416051\n",
      "Iteration 400 : cost 2748.624077563715\n",
      "Iteration 500 : cost 1612.903878697869\n",
      "Iteration 600 : cost 952.4681843193124\n"
     ]
    }
   ],
   "source": [
    "_.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4a09e-1883-4f8b-91bd-b5d2c63c2990",
   "metadata": {},
   "outputs": [],
   "source": [
    "_.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db9598-0e51-48dc-a135-167c3502dc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b992aa1c-9e4f-4594-bd0e-6afb943540be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d27a4-e1e2-424a-8bfe-b6ef020c80b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c19de-8bd4-4370-a7c6-987d99be6dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf27466-e4a8-4539-88b7-7d70dfbbb557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
