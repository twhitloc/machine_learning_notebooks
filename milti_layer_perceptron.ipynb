{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40a7e56-f4eb-4541-8a58-fa9450e45776",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637025ab-74e8-48bc-b116-f19039025bf8",
   "metadata": {},
   "source": [
    "## Fully Connected Feed Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4faa7-757c-41a4-9924-7e6d2dbf7255",
   "metadata": {},
   "source": [
    "### The Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ff2b2-a382-473c-966f-c369b2e00161",
   "metadata": {},
   "source": [
    "Perceptrons are a part of a class of algorithms known as Artificial Neural Networks and they exist inside of the Deep Learning Paradigm, where features and functions are learned by these Artificial Neural Networks from large datasets.  As opposed to traditional Machine Learning approaches the amount of feature engineering and design are drastically reduced as the data structure will internally model these during the training phase.\n",
    "\n",
    "These algorithms use densely connected layers of weights, which are NxM matrices of real numbers, to multiply with the input parameters, the independent variables to predict the labeled outcome which is the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc90db0-ed5e-46df-b041-8ee59c5f08ea",
   "metadata": {},
   "source": [
    "These calculations are performed by stacking together these calculations, passing the output of one layer as the inputs into another layer, through a process known as feed forward.  Once these input variables are multiplied by the individual weights in a given layer, by a process known as dot product, or matrix multiplication, they are then passed into what is known as an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579e82a-5094-4cd2-bbf9-3b6d99cf505b",
   "metadata": {},
   "source": [
    "Activation Functions are non-linear functions that when multiplied by these vector sums produce what is known as an activation bump.  These functions are specifically non-linear primarily because this allows the networks to model very sophisticated functions and mappings through a process known as back-propagation.  This is the process by which the cost / error function is passed back through these series of layers to modify the individual weights and biases such that the error between the prediction and the true label is minimized over time, which allows the data structure to model the underlying latent space between these examples. In other words some prototypical features or representations that define this example space represented by the data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52478954-c9ff-47dc-9bd1-944bfd252b42",
   "metadata": {},
   "source": [
    "## Activation Functions:\n",
    "\n",
    "* Sigmoid : outputs values between 0, 1\n",
    "* Tanh : Outputs  values between -1, 1\n",
    "* Relu : Outputs values between 0, Infinity\n",
    "\n",
    "* The non-linearity allows for more complex functions to represent the space / plane that separates the class distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb086dc6-bed3-48a4-98bd-4c2d6e4cfc1d",
   "metadata": {},
   "source": [
    "## Perceptron is composed of these Stages:\n",
    "* Input (Any transformations on the raw data)\n",
    "* Weights (An NxM Matrix of coefficients to multiply the the input by)\n",
    "* Sum (Weights get added to any bias terms to reduce the likelihood of overfitting)\n",
    "* Non-Linearity (The resulting Matrix sum (Weights + Biases) is multiplied by a Non-Linear Activation Function)\n",
    "* Output (Classification Probability Predictions or Continuous Value Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "769c8a2f-6f9b-4e67-a046-2961852ff342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0eed5a8-7426-49f1-a5dd-04b67f741c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseLayer(layers.Layer):\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.W = self.add_weight(shape=[input_dims,\n",
    "                                        output_dims])\n",
    "        self.b = self.add_weight(shape=[1,\n",
    "                                        output_dims])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        sums = tf.matmul(inputs, self.W) + self.b\n",
    "        \n",
    "        return tf.nn.relu(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca611026-979c-41e1-896f-de3a649cefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(units=2)\n",
    "my_layer = MyDenseLayer(10,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44d87104-dc2f-4266-a71c-ad57588605c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=float32, numpy=\n",
       "array([[0.       , 1.322483 , 0.       , 1.5209744, 1.4172635, 4.02373  ,\n",
       "        2.3878949, 0.       , 0.       , 0.       ],\n",
       "       [0.       , 1.322483 , 0.       , 1.5209744, 1.4172635, 4.02373  ,\n",
       "        2.3878949, 0.       , 0.       , 0.       ],\n",
       "       [0.       , 1.322483 , 0.       , 1.5209744, 1.4172635, 4.02373  ,\n",
       "        2.3878949, 0.       , 0.       , 0.       ],\n",
       "       [0.       , 1.322483 , 0.       , 1.5209744, 1.4172635, 4.02373  ,\n",
       "        2.3878949, 0.       , 0.       , 0.       ],\n",
       "       [0.       , 1.322483 , 0.       , 1.5209744, 1.4172635, 4.02373  ,\n",
       "        2.3878949, 0.       , 0.       , 0.       ],\n",
       "       [0.       , 1.322483 , 0.       , 1.5209744, 1.4172635, 4.02373  ,\n",
       "        2.3878949, 0.       , 0.       , 0.       ],\n",
       "       [0.       , 1.322483 , 0.       , 1.5209744, 1.4172635, 4.02373  ,\n",
       "        2.3878949, 0.       , 0.       , 0.       ],\n",
       "       [0.       , 1.322483 , 0.       , 1.5209744, 1.4172635, 4.02373  ,\n",
       "        2.3878949, 0.       , 0.       , 0.       ],\n",
       "       [0.       , 1.322483 , 0.       , 1.5209744, 1.4172635, 4.02373  ,\n",
       "        2.3878949, 0.       , 0.       , 0.       ],\n",
       "       [0.       , 1.322483 , 0.       , 1.5209744, 1.4172635, 4.02373  ,\n",
       "        2.3878949, 0.       , 0.       , 0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_layer.call([[np.float32(i) \n",
    "                for i in range(10)]\n",
    "               for _ in range (10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5fb74-14c7-401e-86e7-2cdfaeea56f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64d175-db0e-4af0-ae28-6f9e1979d44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc475c2d-1306-4033-91b2-9ee6eb52af2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
