{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40a7e56-f4eb-4541-8a58-fa9450e45776",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637025ab-74e8-48bc-b116-f19039025bf8",
   "metadata": {},
   "source": [
    "## Fully Connected Feed Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4faa7-757c-41a4-9924-7e6d2dbf7255",
   "metadata": {},
   "source": [
    "### The Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ff2b2-a382-473c-966f-c369b2e00161",
   "metadata": {},
   "source": [
    "Perceptrons are a part of a class of algorithms known as Artificial Neural Networks and they exist inside of the Deep Learning Paradigm, where features and functions are learned by these Artificial Neural Networks from large datasets.  As opposed to traditional Machine Learning approaches the amount of feature engineering and design are drastically reduced as the data structure will internally model these during the training phase.\n",
    "\n",
    "These algorithms use densely connected layers of weights, which are NxM matrices of real numbers, to multiply with the input parameters, the independent variables to predict the labeled outcome which is the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc90db0-ed5e-46df-b041-8ee59c5f08ea",
   "metadata": {},
   "source": [
    "These calculations are performed by stacking together these calculations, passing the output of one layer as the inputs into another layer, through a process known as feed forward.  Once these input variables are multiplied by the individual weights in a given layer, by a process known as dot product, or matrix multiplication, they are then passed into what is known as an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579e82a-5094-4cd2-bbf9-3b6d99cf505b",
   "metadata": {},
   "source": [
    "Activation Functions are non-linear functions that when multiplied by these vector sums produce what is known as an activation bump.  These functions are specifically non-linear primarily because this allows the networks to model very sophisticated functions and mappings through a process known as back-propagation.  This is the process by which the cost / error function is passed back through these series of layers to modify the individual weights and biases such that the error between the prediction and the true label is minimized over time, which allows the data structure to model the underlying latent space between these examples. In other words some prototypical features or representations that define this example space represented by the data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52478954-c9ff-47dc-9bd1-944bfd252b42",
   "metadata": {},
   "source": [
    "## Activation Functions:\n",
    "\n",
    "* Sigmoid : outputs values between 0, 1\n",
    "* Tanh : Outputs  values between -1, 1\n",
    "* Relu : Outputs values between 0, Infinity\n",
    "\n",
    "* The non-linearity allows for more complex functions to represent the space / plane that separates the class distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb086dc6-bed3-48a4-98bd-4c2d6e4cfc1d",
   "metadata": {},
   "source": [
    "## Perceptron is composed of these Stages:\n",
    "* Input (Any transformations on the raw data)\n",
    "* Weights (An NxM Matrix of coefficients to multiply the the input by)\n",
    "* Sum (Weights get added to any bias terms to reduce the likelihood of overfitting)\n",
    "* Non-Linearity (The resulting Matrix sum (Weights + Biases) is multiplied by a Non-Linear Activation Function)\n",
    "* Output (Classification Probability Predictions or Continuous Value Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2026e-0a86-4aa0-b8bc-b5eccdd2752b",
   "metadata": {},
   "source": [
    "### Links to resources, tutorials:\n",
    "* https://keras.io/examples/vision/mlp_image_classification/\n",
    "* https://www.tutorialspoint.com/tensorflow/tensorflow_multi_layer_perceptron_learning.htm\n",
    "* https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/1_mlp.ipynb\n",
    "* https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "* https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n",
    "\n",
    "### Weights and Biases\n",
    "* https://wandb.ai/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769c8a2f-6f9b-4e67-a046-2961852ff342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6436eb72-cb23-49a4-aba8-a3ba1382bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2bdf99c-30ef-4a5b-8e6d-3c4b779f43df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 64 X 64 = 4096\n",
      "Patch size: 8 X 8 = 64 \n",
      "Patches per image: 64\n",
      "Elements per patch (3 channels): 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-29 15:44:08.111005: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "dropout_rate = 0.2\n",
    "image_size = 64  # We'll resize input images to this size.\n",
    "patch_size = 8  # Size of the patches to be extracted from the input images.\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
    "embedding_dim = 256  # Number of hidden units.\n",
    "num_blocks = 4  # Number of blocks.\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "\n",
    "\n",
    "augmentations=[layers.Normalization(),\n",
    "               layers.Resizing(image_size, image_size),\n",
    "               layers.RandomFlip('horizontal'),\n",
    "               layers.RandomZoom(height_factor=.2, width_factor=.2)]\n",
    "# Set up operations so that they will occur sequentially\n",
    "data_augmentation = keras.Sequential(augmentations, name='data_augmentation')\n",
    "# Augment the training data examples\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebd703-2187-4dce-bf0c-c256efb9b286",
   "metadata": {},
   "source": [
    "## Keras Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce74106-a0de-4914-b55e-5564027b4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_classifier(blocks, \n",
    "                           positional_encoding=False):\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data\n",
    "    augmented = data_augmentation(inputs)\n",
    "    \n",
    "    # Create image patches / sub-images\n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    \n",
    "    # Encode image patches to create a tensor\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    \n",
    "    # Add the positional embeddings to the image patches\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(input_dim=num_patches, output_dim=embedding_dim)(positions)\n",
    "        x = x + position_embedding\n",
    "    \n",
    "    # Process the encoded input using the Neural Network Blocks\n",
    "    x = blocks(x)\n",
    "    \n",
    "    # Apply global average pooling downsample representation\n",
    "    representation = layers.GlobalAvgPool1D()(x)\n",
    "    \n",
    "    # Apploy dropout to prevent overfitting\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    \n",
    "    # Compute logits, these are the units that lead into a softmax classifier\n",
    "    logits = layers.Dense(units=num_classes)(representation)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=logits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0eed5a8-7426-49f1-a5dd-04b67f741c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, learning_rate):\n",
    "    # Create the optimizer with weight decay, this will perform : weights * (2* (Sum of Squares * weight_decay))\n",
    "    # Described here https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate,\n",
    "                                     weight_decay=weight_decay)\n",
    "    \n",
    "    # Compile the model using the optimizer, metrics and loss\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=[keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
    "                           keras.metrics.SparseTopKCategoricalAccuracy(name='top5-acc')])\n",
    "    \n",
    "    # Create a learning rate schedule callback to slowly dip the learning rate as epochs go on, to help with finding minima\n",
    "    learning_rate_schedule = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "    \n",
    "    # Create early stopping callback to prevent overfitting by stopping early and choosing the best weights\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Fit the model on the dataset using the Optimizer, Loss Function, and other hyperparameters, IE: batch_size, epochs\n",
    "    record = model.fit(x=x_train,\n",
    "                       y=y_train,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=num_epochs,\n",
    "                       validation_split=0.1,\n",
    "                       callbacks=[early_stopping_callback,\n",
    "                                  learning_rate_schedule])\n",
    "    \n",
    "    _, score, top_5_score = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(score * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_score * 100, 2)}%\")\n",
    "\n",
    "    # Return history to plot learning curves.\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d87104-dc2f-4266-a71c-ad57588605c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    \"\"\"\n",
    "    Image Patch Extraction as a Layer extension\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "    def call(self, images):\n",
    "        \"\"\"Apply the patching / sub-sampling of the images and create the sub-images\"\"\"\n",
    "        # Batch size is the first index of the shape [batch_size, image_height, image_width, image_channels]\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        # Generate the sub-regions of the image to feed to the network in pieces\n",
    "        patches = tf.image.extract_patches(images=images,\n",
    "                                           sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "                                           strides=[1, self.patch_size, self.patch_size, 1],\n",
    "                                           rates=[1, 1, 1, 1],\n",
    "                                           padding='VALID')\n",
    "        \n",
    "        patch_dims = patches.shape[-1]\n",
    "        # Reshape so that the image patches are sequenced\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706fdb2-beff-447b-93da-13494764e959",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multi-Layer Perceptron Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9aeae0-7be9-447f-b33b-3807f9854cb6",
   "metadata": {},
   "source": [
    "<P>We will analyze a variety of different architectures all based on the Multi Layer Perceptron or MLP as it is known.<br>\n",
    "These will gradually get more complex and we will compare and contrast them on the task of Image Classification.</P>\n",
    "\n",
    "<b>Model Types</b>\n",
    "* Basic Multi Layer Perceptron Network (MLP)\n",
    "* MLPMixer Network (https://arxiv.org/pdf/2105.01601.pdf)\n",
    "* FNetwork (https://arxiv.org/abs/2105.03824)\n",
    "* gMLP Network (https://arxiv.org/abs/2105.08050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a21ab78b-ede1-4eb4-b1c9-2a2dbc0b042b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1616303530.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [7]\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "weight_decay = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "dropout_rate = 0.2\n",
    "image_size = 64  # We'll resize input images to this size.\n",
    "patch_size = 8  # Size of the patches to be extracted from the input images.\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
    "embedding_dim = 256  # Number of hidden units.\n",
    "num_blocks = 4  # Number of blocks.\n",
    "\n",
    "augmentations=[layers.Normalization(),\n",
    "               layers.Resizing(image_size, image_size),\n",
    "               layers.RandomFlip('horizontal'),\n",
    "               layers.RandomZoom(height_factor=.2, width_factor=.2)]\n",
    "# Set up operations so that they will occur sequentially\n",
    "data_augmentation = keras.Sequential(augmentations, name='data_augmentation')\n",
    "# Augment the training data examples\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n",
    "\"\"\"\n",
    "\n",
    "class MLPLayer(layers.Layer):\n",
    "    def __init__(self, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(MLPLayer, self).__init__(*args, **kwargs)\n",
    "        # 64 * 64 * 3 = 12288\n",
    "        self.mlp1 = keras.Sequential([layers.Dense(units=embedding_dim])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a78d1f-459c-45f5-bf35-040dba2f0118",
   "metadata": {},
   "source": [
    "### MLP-Mixer Model\n",
    "\n",
    "* The MLP-Mixer is an architecture based exclusively on multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
    "\n",
    "        One applied independently to image patches, which mixes the per-location features.\n",
    "        The other applied across patches (along channels), which mixes spatial information.\n",
    "        \n",
    "\n",
    "* This is similar to a depthwise separable convolution based model such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization instead of batch normalization.\n",
    "\n",
    "* https://arxiv.org/abs/2105.01601\n",
    "\n",
    "* The MLP-Mixer model tends to have much less number of parameters compared to convolutional and transformer-based models, which leads to less training and serving computational cost.\n",
    "\n",
    "* As mentioned in the MLP-Mixer paper, when pre-trained on large datasets, or with modern regularization schemes, the MLP-Mixer attains competitive scores to state-of-the-art models. You can obtain better results by increasing the embedding dimensions, increasing, increasing the number of mixer blocks, and training the model for longer. You may also try to increase the size of the input images and use different patch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d64d175-db0e-4af0-ae28-6f9e1979d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPMixerLayer(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 num_patches,\n",
    "                 embedding_dim,\n",
    "                 dropout_rate,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create the first of the different blocks (sub-MLPs) input size=num_patches, output_size=num_patches\n",
    "        self.mlp1 = keras.Sequential([layers.Dense(units=num_patches),\n",
    "                                      tfa.layers.GELU(), # Gaussian Error Linear Unit\n",
    "                                      layers.Dense(units=num_patches),\n",
    "                                      layers.Dropout(rate=dropout_rate)])\n",
    "        \n",
    "        # Create the second of the different blocks input_size=num_blocks, output_size=embedding_dim\n",
    "        self.mlp2 = keras.Sequential([layers.Dense(units=num_patches),\n",
    "                                      tfa.layers.GELU(),\n",
    "                                      layers.Dense(units=embedding_dim),\n",
    "                                      layers.Dropout(rate=dropout_rate)])\n",
    "        # Normalize using LayerNorm so that there is no dependency on batch_size while reducing overfitting\n",
    "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Apply input normalization\n",
    "        x = self.normalize(inputs)\n",
    "        \n",
    "        # Transpose inputs\n",
    "        x_channels = tf.linalg.matrix_transpose(x)\n",
    "        \n",
    "        # Apply block1 to the input channels, each independently\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
    "        # Add a skip-connection to allow strong input features to have additional input\n",
    "        x = mlp1_outputs + inputs\n",
    "        # Normalize Layers\n",
    "        x_patches = self.normalize(x)\n",
    "        mlp2_outputs = self.mlp2(x_patches)\n",
    "        # Skip Connection\n",
    "        x = x + mlp2_outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d771c-f2b0-4771-8c91-3348f6236dc5",
   "metadata": {},
   "source": [
    "### The FNet model\n",
    "\n",
    "* The FNet uses a similar block to the Transformer block. However, FNet replaces the self-attention layer in the Transformer block with a parameter-free 2D Fourier transformation layer:\n",
    "\n",
    "    *    One 1D Fourier Transform is applied along the patches.\n",
    "    *    One 1D Fourier Transform is applied along the channels.\n",
    "\n",
    "* As shown in the FNet paper, better results can be achieved by increasing the embedding dimensions, increasing the number of FNet blocks, and training the model for longer. You may also try to increase the size of the input images and use different patch sizes. The FNet scales very efficiently to long inputs, runs much faster than attention-based Transformer models, and produces competitive accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36d4b656-9f8f-4b83-a4a4-a7bb07235697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNETLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(FNETLayer, self).__init__(*args, **kwargs)\n",
    "        # Define the transformer block\n",
    "        self.ffn = keras.Sequential([layers.Dense(units=embedding_dim), # Dense layer for transforming to the embedding_dim\n",
    "                                     tfa.layers.GELU(), # Gaussian Error Linear Unit\n",
    "                                     layers.Dropout(rate=dropout_rate), # Dropout to prevent over-fitting\n",
    "                                     layers.Dense(units=embedding_dim)]) # Another dense transformer layer\n",
    "        \n",
    "        # Layer Normalizations\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Transform inputs using a FFT\n",
    "        x = tf.cast(tf.signal.fft2d(tf.cast(inputs, dtype=tf.dtypes.complex64)),\n",
    "                    dtype=tf.dtypes.float32)\n",
    "        \n",
    "        # Skip connection for residual signal\n",
    "        x = x + inputs\n",
    "        # Apply layer normalization \n",
    "        x = self.layer_norm1(x)\n",
    "        # Apply the transformer network block\n",
    "        x_ffn = self.ffn(x)\n",
    "        # Apply another skip connection for residual signal\n",
    "        x = x + x_ffn\n",
    "        x = self.layer_norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95553d6b-2d00-4552-ae92-0edcca066ebf",
   "metadata": {},
   "source": [
    "### The gMLP model\n",
    "\n",
    "* The gMLP is a MLP architecture that features a Spatial Gating Unit (SGU). The SGU enables cross-patch interactions across the spatial (channel) dimension, by:\n",
    "\n",
    "    *    Transforming the input spatially by applying linear projection across patches (along channels).\n",
    "    *    Applying element-wise multiplication of the input and its spatial transformation.\n",
    "\n",
    "* As shown in the gMLP paper, better results can be achieved by increasing the embedding dimensions, increasing the number of gMLP blocks, and training the model for longer. You may also try to increase the size of the input images and use different patch sizes. Note that, the paper used advanced regularization strategies, such as MixUp and CutMix, as well as AutoAugment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64b9f9cc-f422-404b-8518-d46da7290540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gMLPLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        \n",
    "        super(gMLPLayer, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.channel_projection1 = keras.Sequential([layers.Dense(units=embedding_dim * 2),\n",
    "                                                     tfa.layers.GELU(),\n",
    "                                                     layers.Dropout(rate=dropout_rate)])\n",
    "        \n",
    "        self.channel_projection2 = layers.Dense(units=embedding_dim)\n",
    "        \n",
    "        self.spatial_projection = layers.Dense(units=num_patches, bias_initializer='Ones')\n",
    "        \n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def spatial_gating_unit(self, x):\n",
    "        u, v = tf.split(x, num_or_size_splits=2, axis=2)\n",
    "        v = self.layer_norm2(v)\n",
    "        v_channels = tf.linalg.matrix_transpose(v)\n",
    "        v_projected = self.spatial_projection(v_channels)\n",
    "        v_projected = tf.linalg.matrix_transpose(v_projected)\n",
    "        return u * v_projected\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Layer Normalization\n",
    "        x = self.layer_norm1(inputs)\n",
    "        # Chanel Projection\n",
    "        x_projected = self.channel_projection1(x)\n",
    "        # Spatial Channel Projection Mixture 1\n",
    "        x_spatial = self.spatial_gating_unit(x_projected)\n",
    "        # Spatial Channel Projection Mixture 2\n",
    "        x_projected = self.channel_projection2(x_spatial)\n",
    "        # Apply skip connection for residual features\n",
    "        return x + x_projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37457c4a-cae1-40b3-987c-df0187a34f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1407/1407 [==============================] - 192s 134ms/step - loss: 4.0311 - acc: 0.0870 - top5-acc: 0.2612 - val_loss: 3.6081 - val_acc: 0.1436 - val_top5-acc: 0.3936 - lr: 3.0000e-04\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 174s 124ms/step - loss: 3.4988 - acc: 0.1627 - top5-acc: 0.4211 - val_loss: 3.3142 - val_acc: 0.2008 - val_top5-acc: 0.4792 - lr: 3.0000e-04\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 184s 131ms/step - loss: 3.2791 - acc: 0.2037 - top5-acc: 0.4834 - val_loss: 3.1515 - val_acc: 0.2272 - val_top5-acc: 0.5200 - lr: 3.0000e-04\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 182s 130ms/step - loss: 3.1205 - acc: 0.2343 - top5-acc: 0.5257 - val_loss: 3.0067 - val_acc: 0.2564 - val_top5-acc: 0.5542 - lr: 3.0000e-04\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 179s 127ms/step - loss: 2.9859 - acc: 0.2575 - top5-acc: 0.5608 - val_loss: 2.8926 - val_acc: 0.2724 - val_top5-acc: 0.5814 - lr: 3.0000e-04\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 197s 140ms/step - loss: 2.8802 - acc: 0.2777 - top5-acc: 0.5878 - val_loss: 2.8075 - val_acc: 0.2942 - val_top5-acc: 0.6092 - lr: 3.0000e-04\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 198s 141ms/step - loss: 2.8093 - acc: 0.2897 - top5-acc: 0.6064 - val_loss: 2.7425 - val_acc: 0.3036 - val_top5-acc: 0.6248 - lr: 3.0000e-04\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 177s 126ms/step - loss: 2.7516 - acc: 0.3039 - top5-acc: 0.6177 - val_loss: 2.7223 - val_acc: 0.3138 - val_top5-acc: 0.6298 - lr: 3.0000e-04\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 177s 126ms/step - loss: 2.7152 - acc: 0.3119 - top5-acc: 0.6265 - val_loss: 2.6628 - val_acc: 0.3222 - val_top5-acc: 0.6446 - lr: 3.0000e-04\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 176s 125ms/step - loss: 2.6901 - acc: 0.3173 - top5-acc: 0.6312 - val_loss: 2.6856 - val_acc: 0.3156 - val_top5-acc: 0.6376 - lr: 3.0000e-04\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 2.6229 - acc: 0.3367 - top5-acc: 0.6481\n",
      "Test accuracy: 33.67%\n",
      "Test top 5 accuracy: 64.81%\n",
      "Epoch 1/10\n",
      "1407/1407 [==============================] - 470s 332ms/step - loss: 4.1322 - acc: 0.0738 - top5-acc: 0.2326 - val_loss: 3.8413 - val_acc: 0.1078 - val_top5-acc: 0.3142 - lr: 3.0000e-04\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 493s 351ms/step - loss: 3.7197 - acc: 0.1286 - top5-acc: 0.3521 - val_loss: 3.5659 - val_acc: 0.1574 - val_top5-acc: 0.4026 - lr: 3.0000e-04\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 469s 333ms/step - loss: 3.5252 - acc: 0.1596 - top5-acc: 0.4116 - val_loss: 3.4235 - val_acc: 0.1816 - val_top5-acc: 0.4464 - lr: 3.0000e-04\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 451s 321ms/step - loss: 3.3897 - acc: 0.1846 - top5-acc: 0.4525 - val_loss: 3.2768 - val_acc: 0.2086 - val_top5-acc: 0.4880 - lr: 3.0000e-04\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 449s 319ms/step - loss: 3.2841 - acc: 0.2034 - top5-acc: 0.4814 - val_loss: 3.1921 - val_acc: 0.2232 - val_top5-acc: 0.5068 - lr: 3.0000e-04\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 449s 319ms/step - loss: 3.2200 - acc: 0.2171 - top5-acc: 0.5001 - val_loss: 3.1497 - val_acc: 0.2324 - val_top5-acc: 0.5202 - lr: 3.0000e-04\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 450s 320ms/step - loss: 3.1782 - acc: 0.2230 - top5-acc: 0.5135 - val_loss: 3.1405 - val_acc: 0.2370 - val_top5-acc: 0.5160 - lr: 3.0000e-04\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 449s 319ms/step - loss: 3.1559 - acc: 0.2274 - top5-acc: 0.5138 - val_loss: 3.1115 - val_acc: 0.2404 - val_top5-acc: 0.5384 - lr: 3.0000e-04\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 449s 319ms/step - loss: 3.1363 - acc: 0.2326 - top5-acc: 0.5230 - val_loss: 3.0730 - val_acc: 0.2526 - val_top5-acc: 0.5384 - lr: 3.0000e-04\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 449s 319ms/step - loss: 3.1170 - acc: 0.2358 - top5-acc: 0.5279 - val_loss: 3.0738 - val_acc: 0.2414 - val_top5-acc: 0.5338 - lr: 3.0000e-04\n",
      "313/313 [==============================] - 41s 132ms/step - loss: 3.0707 - acc: 0.2365 - top5-acc: 0.5438\n",
      "Test accuracy: 23.65%\n",
      "Test top 5 accuracy: 54.38%\n",
      "Epoch 1/10\n",
      "1407/1407 [==============================] - 260s 182ms/step - loss: 3.9724 - acc: 0.0952 - top5-acc: 0.2818 - val_loss: 3.5367 - val_acc: 0.1564 - val_top5-acc: 0.4218 - lr: 3.0000e-04\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 256s 182ms/step - loss: 3.4220 - acc: 0.1782 - top5-acc: 0.4414 - val_loss: 3.2418 - val_acc: 0.2092 - val_top5-acc: 0.4944 - lr: 3.0000e-04\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 257s 183ms/step - loss: 3.1212 - acc: 0.2333 - top5-acc: 0.5242 - val_loss: 2.9174 - val_acc: 0.2658 - val_top5-acc: 0.5836 - lr: 3.0000e-04\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 257s 183ms/step - loss: 2.8728 - acc: 0.2770 - top5-acc: 0.5858 - val_loss: 2.7298 - val_acc: 0.3026 - val_top5-acc: 0.6226 - lr: 3.0000e-04\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 258s 183ms/step - loss: 2.7121 - acc: 0.3130 - top5-acc: 0.6238 - val_loss: 2.6337 - val_acc: 0.3280 - val_top5-acc: 0.6406 - lr: 3.0000e-04\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 258s 184ms/step - loss: 2.6232 - acc: 0.3309 - top5-acc: 0.6477 - val_loss: 2.6525 - val_acc: 0.3236 - val_top5-acc: 0.6346 - lr: 3.0000e-04\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 258s 184ms/step - loss: 2.5482 - acc: 0.3448 - top5-acc: 0.6628 - val_loss: 2.5174 - val_acc: 0.3534 - val_top5-acc: 0.6694 - lr: 3.0000e-04\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 259s 184ms/step - loss: 2.4987 - acc: 0.3545 - top5-acc: 0.6733 - val_loss: 2.4883 - val_acc: 0.3626 - val_top5-acc: 0.6730 - lr: 3.0000e-04\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 259s 184ms/step - loss: 2.4590 - acc: 0.3626 - top5-acc: 0.6832 - val_loss: 2.4733 - val_acc: 0.3596 - val_top5-acc: 0.6830 - lr: 3.0000e-04\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 260s 185ms/step - loss: 2.4273 - acc: 0.3697 - top5-acc: 0.6896 - val_loss: 2.5567 - val_acc: 0.3440 - val_top5-acc: 0.6644 - lr: 3.0000e-04\n",
      "313/313 [==============================] - 18s 59ms/step - loss: 2.5095 - acc: 0.3555 - top5-acc: 0.6745\n",
      "Test accuracy: 35.55%\n",
      "Test top 5 accuracy: 67.45%\n"
     ]
    }
   ],
   "source": [
    "model_blocks= [\n",
    "    keras.Sequential([MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]), # mlpmixernet\n",
    "    keras.Sequential([FNETLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]), # Fnet\n",
    "    keras.Sequential([gMLPLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)])\n",
    "]\n",
    "learning_rates = [0.0003] * len(model_blocks)\n",
    "classifiers = []\n",
    "records = []\n",
    "\n",
    "for learning_rate, mb in zip(learning_rates, model_blocks):\n",
    "    classifiers.append(build_keras_classifier(mb))\n",
    "    records.append(run_experiment(classifiers[-1], learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774818a2-24ff-4e82-bfde-ba74f78f19d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b808a7-c262-4f81-a6dc-e1b69c1f8239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1cfb3-34f7-4e0c-b0d6-2a83196947d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
